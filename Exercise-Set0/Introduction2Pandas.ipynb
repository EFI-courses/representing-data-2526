{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ed66a387",
   "metadata": {},
   "source": [
    "# An introduction to Pandas\n",
    "\n",
    "[pandas](http://pandas.pydata.org) is a module which allows the construction of a *dataframe*, this is an object to store data that looks a little like a spreadsheet (the data is indexed principally by a column name and row name/number). It also includes functions designed to make working with this structured or tabular data fast and easy.  \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Again, the website for Pandas is good and contains the main set of documentation. It is a harder module to understand, and the documentation on the website is more dense to read. [Here](http://pandas.pydata.org/index.html) is the main website. \n",
    "- The main documentation for Pandas is [here](http://pandas.pydata.org/pandas-docs/stable/).\n",
    "- There is a quick introduction to Pandas [here](http://pandas.pydata.org/pandas-docs/stable/10min.html)\n",
    "- There is a fantastic tutorial (also in Jupyter) [here](http://pandas.pydata.org/pandas-docs/stable/tutorials.html) (under Lessons for New Pandas Users). This is well worth working through a little if you want a longer introduction to the basic concepts in Pandas.\n",
    "\n",
    "Here are some basic examples to getting started with pandas, the datasets we use need to be copied into the same location as this notebook in order to use them in our code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3702dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common pandas import statement\n",
    "import pandas as pd\n",
    "\n",
    "# Some other related necessary packages to import most of the time \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c282320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B         C\n",
      "0  0.941448  0.642557  0.263331\n",
      "1  0.056967  0.399789  0.021823\n",
      "2  0.790075  0.693526  0.475417\n",
      "3  0.215275  0.084278  0.700621\n",
      "4  0.079142  0.589419  0.310219\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creating a DataFrame with random data\n",
    "data = np.random.rand(5, 3)\n",
    "df = pd.DataFrame(data, columns=['A', 'B', 'C'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4c4f0",
   "metadata": {},
   "source": [
    "## Removing columns/indices <a name=\"removing\"></a>\n",
    "Similar to above, it is easy to remove entries. This is done with the `drop()` method and can be applied to both columns and indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f87ecea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Edinburgh  Glasgow  Dundee\n",
      "a          0        1       2\n",
      "b          3        4       5\n",
      "c          6        7       8\n"
     ]
    }
   ],
   "source": [
    "# define new DataFrame using a function from NumPy (don't worry about what this line does just now)\n",
    "data = np.reshape(np.arange(9), (3,3))\n",
    "\n",
    "df = pd.DataFrame(data, index=['a','b','c'],\n",
    "                  columns=['Edinburgh', 'Glasgow', 'Dundee'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767ce56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Edinburgh</th>\n",
       "      <th>Glasgow</th>\n",
       "      <th>Dundee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Edinburgh  Glasgow  Dundee\n",
       "a          0        1       2\n",
       "c          6        7       8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('b')  # remove row (index)~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f76ae15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Edinburgh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Edinburgh\n",
       "a          0\n",
       "b          3\n",
       "c          6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also drop from a column - here is the way we would drop two columns:\n",
    "df.drop(['Dundee', 'Glasgow'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae355d1",
   "metadata": {},
   "source": [
    "Note that the original data frame is unchanged: `df.drop()` gives us a new data frame with the desired data dropped, and leaves the original data intact. We can ask `.drop()` to operate directly on the original data frame by setting the argument `inplace=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e99c16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Edinburgh  Glasgow  Dundee\n",
      "a          0        1       2\n",
      "b          3        4       5\n",
      "c          6        7       8\n",
      "-----\n",
      "   Edinburgh  Glasgow  Dundee\n",
      "b          3        4       5\n",
      "c          6        7       8\n"
     ]
    }
   ],
   "source": [
    "# Original data frame that we created\n",
    "print(df)\n",
    "\n",
    "# Dropping the row called a and updating the df using \"inplace=True\"\n",
    "df.drop('a', inplace=True)\n",
    "print('-----')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167ae3b",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "Rows and columns in a dataframe can be referred to by name (if they are given a name - for the example at the beginning using oil reserves we didn't specify row names), or by row/column number. \n",
    "\n",
    "**Important Note** - python starts counting from 0, i.e. 0, 1, 2, ... as indexing, not starts from 1 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17aabd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Edinburgh  Glasgow  Dundee\n",
      "a          0        1       2\n",
      "b          3        4       5\n",
      "c          6        7       8\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data, index=['a','b','c'],\n",
    "                  columns=['Edinburgh', 'Glasgow', 'Dundee'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae09c43",
   "metadata": {},
   "source": [
    "We can get the 'a' row by name or by knowing that it is row 0 using the following two pandas functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "060de81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edinburgh    0\n",
      "Glasgow      1\n",
      "Dundee       2\n",
      "Name: a, dtype: int64\n",
      "-----\n",
      "Edinburgh    0\n",
      "Glasgow      1\n",
      "Dundee       2\n",
      "Name: a, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Note where the square brackets are!\n",
    "print(df.loc['a'])\n",
    "print('-----')\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8b46f",
   "metadata": {},
   "source": [
    "We can also grab two rows at a time by listing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb0138b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Edinburgh  Glasgow  Dundee\n",
      "a          0        1       2\n",
      "b          3        4       5\n",
      "-----\n",
      "   Edinburgh  Glasgow  Dundee\n",
      "a          0        1       2\n",
      "b          3        4       5\n"
     ]
    }
   ],
   "source": [
    "# Again note where the square brackets are!\n",
    "print(df.loc[['a','b']])\n",
    "print('-----')\n",
    "print(df.iloc[[0,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a96496",
   "metadata": {},
   "source": [
    "Likewise we can do similar things with **column names** and **numbers** and **multiple column names and numbers** \n",
    "- Here all in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "015b3878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    0\n",
      "b    3\n",
      "c    6\n",
      "Name: Edinburgh, dtype: int64\n",
      "-----\n",
      "a    0\n",
      "b    3\n",
      "c    6\n",
      "Name: Edinburgh, dtype: int64\n",
      "=====\n",
      "   Edinburgh  Dundee\n",
      "a          0       2\n",
      "b          3       5\n",
      "c          6       8\n",
      "-----\n",
      "   Edinburgh  Dundee\n",
      "a          0       2\n",
      "b          3       5\n",
      "c          6       8\n"
     ]
    }
   ],
   "source": [
    "print(df['Edinburgh'])\n",
    "print('-----')\n",
    "\n",
    "print(df.iloc[:,0])\n",
    "print('=====')\n",
    "\n",
    "print(df[['Edinburgh','Dundee']])\n",
    "print('-----')\n",
    "\n",
    "print(df.iloc[:,[0,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aea787",
   "metadata": {},
   "source": [
    "## Summarizing and computing descriptive stats\n",
    "`pandas` is equipped with common mathematical and statistical methods. Most of which fall into the category of reductions or summary statistics. These are functions that extract a single value from a list of values. For example, you can extract the sum of a column or row like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f458146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Edinburgh  Glasgow  Dundee\n",
      "a          0        1       2\n",
      "b          3        4       5\n",
      "c          6        7       8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Edinburgh     9\n",
       "Glasgow      12\n",
       "Dundee       15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df)\n",
    "\n",
    "df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d94354",
   "metadata": {},
   "source": [
    "Notice how that created the sum of each column?\n",
    "\n",
    "We can sum across rows by adding an extra option to `sum()`\n",
    "   - using `axis='columns'` to get row sum\n",
    "   - using `axis='rows'` to get column sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "600e21f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a     3\n",
       "b    12\n",
       "c    21\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sum(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e527d919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edinburgh     9\n",
       "Glasgow      12\n",
       "Dundee       15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sum(axis='rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02ff18",
   "metadata": {},
   "source": [
    "A similar method also exists for obtaining the mean of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6e99d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edinburgh    3.0\n",
       "Glasgow      4.0\n",
       "Dundee       5.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the mean of the values over the requested axis, by default axis=0\n",
    "df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e0162",
   "metadata": {},
   "source": [
    "It is also possible to get an overall statistical description of the dataframe by using the `describe()` function. It is really crucial to understand the meaning of each row! We will talk about these measures regularly during the semester. Just to give a general idea, please look at the following diagram:\n",
    "\n",
    "[![](https://mermaid.ink/img/pako:eNqtlEGPojAUx79K08nc0FAqKN1kkxWvXnYme1iYQ6UPacSWlDIja_zuWwHdcW4bLQlp__33995r0nfEuRaAGd4aXpfodfUtU8iNH-kKmtzI2sp3QC-WW9lYmTdvaDL5jpbpGnjTGkC6QAkoa3iFXkEJUHn3NhJ6Z_LZ-Ysb6UBajZbhvxyQ5Oy87Ixa4DQhv6o0XbucbxjJEIykP7naXrZGMUj7uCr_otPUlaUENwKt4P2a2OB5fnZiIRWgXFfaNB7a6EogCwfrIXcKbSqe71ChlR0cbmrQBqwFgwxwwTeykrYbaI3tKnA3UsiqYk9FeP68xhq9A_bk-_44n3xIYUsW1AfvDJ58gNyWlp0jD0IfqT_hxmf08oKO81CQK5pSejc6GdF5CKSIH4C-yZtcEu_HDf1_ryB4HIreibq5v8eVmDyuxOT-ErGH92D2XArXPo5neIZtCXvIMHNT97B2Gc7Uyfl4a_VLp3LMrGnBw0a32xKzgleNW7W14BZWkrsetL-q7uFbbdZDd-qblIdrrn5r_c_j1pgd8QEzQqdxRII4jkIyp-FsMfdwh1kYTGeELhZ-NHdbsT8_efhPTyBTSomrhEZ-5C_ILDz9BSZbmoo?type=png)](https://mermaid.live/edit#pako:eNqtlEGPojAUx79K08nc0FAqKN1kkxWvXnYme1iYQ6UPacSWlDIja_zuWwHdcW4bLQlp__33995r0nfEuRaAGd4aXpfodfUtU8iNH-kKmtzI2sp3QC-WW9lYmTdvaDL5jpbpGnjTGkC6QAkoa3iFXkEJUHn3NhJ6Z_LZ-Ysb6UBajZbhvxyQ5Oy87Ixa4DQhv6o0XbucbxjJEIykP7naXrZGMUj7uCr_otPUlaUENwKt4P2a2OB5fnZiIRWgXFfaNB7a6EogCwfrIXcKbSqe71ChlR0cbmrQBqwFgwxwwTeykrYbaI3tKnA3UsiqYk9FeP68xhq9A_bk-_44n3xIYUsW1AfvDJ58gNyWlp0jD0IfqT_hxmf08oKO81CQK5pSejc6GdF5CKSIH4C-yZtcEu_HDf1_ryB4HIreibq5v8eVmDyuxOT-ErGH92D2XArXPo5neIZtCXvIMHNT97B2Gc7Uyfl4a_VLp3LMrGnBw0a32xKzgleNW7W14BZWkrsetL-q7uFbbdZDd-qblIdrrn5r_c_j1pgd8QEzQqdxRII4jkIyp-FsMfdwh1kYTGeELhZ-NHdbsT8_efhPTyBTSomrhEZ-5C_ILDz9BSZbmoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c25c0ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Edinburgh</th>\n",
       "      <th>Glasgow</th>\n",
       "      <th>Dundee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Edinburgh  Glasgow  Dundee\n",
       "count        3.0      3.0     3.0\n",
       "mean         3.0      4.0     5.0\n",
       "std          3.0      3.0     3.0\n",
       "min          0.0      1.0     2.0\n",
       "25%          1.5      2.5     3.5\n",
       "50%          3.0      4.0     5.0\n",
       "75%          4.5      5.5     6.5\n",
       "max          6.0      7.0     8.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate descriptive statistics.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92411a",
   "metadata": {},
   "source": [
    "## Data Loading and Inspection\n",
    "\n",
    "- Clearly one thing we will need to do often is to access datasets and load them into python in order to work with them. \n",
    "- A pandas dataframe is a really convenient way of storing data within python and manipulating it. \n",
    "- Thankfully pandas also has functions which help load in various different standard data storage formats. \n",
    "\n",
    "We'll come back to this topic later - but for now we will just introduce two ways to load data, contained in csv and excel formats.\n",
    "\n",
    "`.csv` is a very common and portable data format. It is an easy to read file format which is usually visualised like a spreadsheet. The data itself is usually separated with a `,` which is called the **delimiter** separating enrties between the items to appear in each column, the next row is then simply on the next line.\n",
    "\n",
    "Let's now see how we can load this data and analyse it - we have already seen this line before above - but here it is again reading in a `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea97cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "reserves = pd.read_csv('oil_reserve_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861c9e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year/Month  Germany  Denmark  Belgium  Bulgaria\n",
      "0     2017M07    20056     1244     3501      1041\n",
      "1     2017M06    20056     1244     3501      1041\n",
      "2     2017M05    20056     1244     3501      1041\n",
      "3     2017M04    20056     1244     3501      1041\n",
      "4     2017M03    19878     1207     4731       992\n",
      "5     2017M02    19878     1207     4731       992\n",
      "6     2017M01    19878     1207     4731       992\n",
      "7     2016M12    19823     1203     4718       989\n",
      "8     2016M11    19823     1203     4718       989\n",
      "9     2016M10    19823     1203     4718       989\n",
      "10    2016M09    19823     1203     4718       989\n",
      "11    2016M08    19823     1203     4718       989\n",
      "12    2016M07    19823     1203     4718       989\n",
      "13    2016M06    19823     1203     4718       989\n",
      "14    2016M05    19823     1203     4718       989\n",
      "15    2016M04    19823     1203     4718       989\n",
      "16    2016M03    19389     1196     3410       929\n",
      "17    2016M02    19389     1196     3410       929\n",
      "18    2016M01    19389     1196     3410       929\n",
      "19    2015M12    19442     1199     3420       931\n",
      "20    2015M11    19442     1199     3420       931\n",
      "21    2015M10    19442     1199     3420       931\n",
      "22    2015M09    19442     1199     3420       931\n",
      "23    2015M08    19442     1199     3420       931\n",
      "24    2015M07    19442     1199     3420       931\n",
      "25    2015M06    19442     1199     3420       931\n",
      "26    2015M05    19442     1199     3420       931\n",
      "27    2015M04    19442     1199     3420       931\n",
      "28    2015M03    20455     1222     3470       869\n",
      "29    2015M02    20455     1222     3470       869\n",
      "30    2015M01    20455     1222     3470       869\n",
      "31    2014M12    20455     1222     3470       869\n",
      "32    2014M11    20455     1222     3470       869\n",
      "33    2014M10    20455     1222     3470       869\n",
      "34    2014M09    20455     1222     3470       869\n",
      "35    2014M08    20455     1222     3470       869\n",
      "36    2014M07    20455     1222     3470       869\n",
      "37    2014M06    20455     1222     3470       869\n",
      "38    2014M05    20455     1222     3470       869\n",
      "39    2014M04    20455     1223     3470       869\n",
      "40    2014M03    20050     1240     4318       918\n",
      "41    2014M02    20050     1240     4318       918\n",
      "42    2014M01    20050     1240     4318       918\n",
      "43    2013M12    20050     1240     4318       918\n",
      "44    2013M11    20050     1240     4318       918\n",
      "45    2013M10    20050     1240     4318       918\n",
      "46    2013M09    20050     1240     4318       918\n",
      "47    2013M08    20050     1240     4318       918\n",
      "48    2013M07    20050     1240     4318       918\n",
      "49    2013M06    20050     1240     4318       918\n",
      "50    2013M05    20050     1240     4318       918\n",
      "51    2013M04    20050     1240     4318       918\n",
      "52    2013M03    23201     1285     4417       828\n",
      "53    2013M02    23201     1285     4417       828\n",
      "54    2013M01    23201     1285     4417       828\n"
     ]
    }
   ],
   "source": [
    "print(reserves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e96cd3",
   "metadata": {},
   "source": [
    "For reading in an excel file there is a similar `read_excel()` function we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9570c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Sawnwood  Woodbased panels  Other wood  Wood pellets  Paper  Pulp  \\\n",
      "0  2013      5488              2964        1267          3432   5929  1100   \n",
      "1  2014      6425              3260        1329          4773   5949  1234   \n",
      "2  2015      6323              3215        1378          6573   6032  1223   \n",
      "3  2016      6646              3410        1208          6782   5876  1092   \n",
      "4  2017      7580              3826        1681          6886   5604  1081   \n",
      "\n",
      "   Recovered paper  Total Pulp and Paper  \n",
      "0              184                  7213  \n",
      "1              136                  7319  \n",
      "2              305                  7560  \n",
      "3              125                  7092  \n",
      "4              107                  6792  \n"
     ]
    }
   ],
   "source": [
    "wood_imports = pd.read_excel('UK_wood_imports.xlsx')\n",
    "print(wood_imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef5ecf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype\n",
      "---  ------                --------------  -----\n",
      " 0   Year                  5 non-null      int64\n",
      " 1   Sawnwood              5 non-null      int64\n",
      " 2   Woodbased panels      5 non-null      int64\n",
      " 3   Other wood            5 non-null      int64\n",
      " 4   Wood pellets          5 non-null      int64\n",
      " 5   Paper                 5 non-null      int64\n",
      " 6   Pulp                  5 non-null      int64\n",
      " 7   Recovered paper       5 non-null      int64\n",
      " 8   Total Pulp and Paper  5 non-null      int64\n",
      "dtypes: int64(9)\n",
      "memory usage: 488.0 bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Getting a concise summary of the DataFrame\n",
    "print(wood_imports.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aabfab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Year    Sawnwood  Woodbased panels   Other wood  Wood pellets  \\\n",
      "count     5.000000     5.00000          5.000000     5.000000      5.000000   \n",
      "mean   2015.000000  6492.40000       3335.000000  1372.600000   5689.200000   \n",
      "std       1.581139   749.75416        317.951254   183.905682   1528.235813   \n",
      "min    2013.000000  5488.00000       2964.000000  1208.000000   3432.000000   \n",
      "25%    2014.000000  6323.00000       3215.000000  1267.000000   4773.000000   \n",
      "50%    2015.000000  6425.00000       3260.000000  1329.000000   6573.000000   \n",
      "75%    2016.000000  6646.00000       3410.000000  1378.000000   6782.000000   \n",
      "max    2017.000000  7580.00000       3826.000000  1681.000000   6886.000000   \n",
      "\n",
      "             Paper         Pulp  Recovered paper  Total Pulp and Paper  \n",
      "count     5.000000     5.000000         5.000000              5.000000  \n",
      "mean   5878.000000  1146.000000       171.400000           7195.200000  \n",
      "std     163.124186    75.713275        79.939352            283.700018  \n",
      "min    5604.000000  1081.000000       107.000000           6792.000000  \n",
      "25%    5876.000000  1092.000000       125.000000           7092.000000  \n",
      "50%    5929.000000  1100.000000       136.000000           7213.000000  \n",
      "75%    5949.000000  1223.000000       184.000000           7319.000000  \n",
      "max    6032.000000  1234.000000       305.000000           7560.000000  \n"
     ]
    }
   ],
   "source": [
    "# Descriptive statistics of the DataFrame\n",
    "print(wood_imports.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6589b6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Sawnwood  Woodbased panels  Other wood  Wood pellets  Paper  Pulp  \\\n",
      "0  2013      5488              2964        1267          3432   5929  1100   \n",
      "1  2014      6425              3260        1329          4773   5949  1234   \n",
      "2  2015      6323              3215        1378          6573   6032  1223   \n",
      "3  2016      6646              3410        1208          6782   5876  1092   \n",
      "4  2017      7580              3826        1681          6886   5604  1081   \n",
      "\n",
      "   Recovered paper  Total Pulp and Paper  \n",
      "0              184                  7213  \n",
      "1              136                  7319  \n",
      "2              305                  7560  \n",
      "3              125                  7092  \n",
      "4              107                  6792  \n"
     ]
    }
   ],
   "source": [
    "# Displaying the first few rows of the DataFrame\n",
    "print(wood_imports.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47235c6e",
   "metadata": {},
   "source": [
    "- The `read_csv` function has a lot of optional arguments (more than 50). It's impossible to memorise all of them - it's usually best just to look up the particular functionality when you need it. \n",
    "\n",
    "- You can search `pandas read_csv` online and find all of the documentation.\n",
    "\n",
    "There are also many other functions that can read textual data. Here are some of them:\n",
    "\n",
    "| Function | Description\n",
    "| -- | -- |\n",
    "| read_csv       | Load delimited data from a file, URL, or file-like object. The default delimiter is a comma `,` |\n",
    "| read_table     | Load delimited data from a file, URL, or file-like object. The default delimiter is tab `\\t` |\n",
    "| read_clipboard | Reads the last object you have copied (Ctrl-C) |\n",
    "| read_excel     | Read tabular data from Excel XLS or XLSX file |\n",
    "| read_hdf       | Read HDF5 file written by pandas |\n",
    "| read_html      | Read all tables found in the given HTML document |\n",
    "| read_json      | Read data from a JSON string representation |\n",
    "| read_sql       | Read the results of a SQL query |\n",
    "\n",
    "**Note: There are also other loading functions which are not touched upon here**\n",
    "\n",
    "We will talk about some of these in a later notebook to give you some examples of different data types and how to work with them and read them into a dataframe for manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fba4a1",
   "metadata": {},
   "source": [
    "## Data Cleaning \n",
    "\n",
    "While planning and constructing a data analysis, a significant amount of time is spent on data preparation: loading, cleaning, transforming and rearranging. \n",
    "\n",
    "- Often the way the data is stored in files isn't in the correct format and needs to be modified. \n",
    "- Researchers usually do this on an ad-hoc basis using programming languages like Python, we will also be talking about examples of this where we simply do some of this preparation within Excel.\n",
    "\n",
    "Here we will discuss some of the pandas tools available for handling missing data, duplicate data, string manipulation, and some other analytical data transformations.\n",
    "\n",
    "### Handling missing data \n",
    "Missing data occurs commonly in many data analysis applications. One of the goals of pandas is to make working with missing data as painless as possible.\n",
    "\n",
    "In pandas, missing numeric data is represented by `NaN` (Not a Number) and can easily be handled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca5e67ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# define new DataFrame using a function from NumPy (don't worry about what this line does just now)\n",
    "df_new = np.reshape(np.arange(9), (3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1396af6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "print(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1493e137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Edinburgh  Glasgow  Dundee\n",
      "a          0        1       2\n",
      "b          3        4       5\n",
      "c          6        7       8\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# Here is our example dataframe again:\n",
    "df_new = pd.DataFrame(data, index=['a','b','c'],\n",
    "                  columns=['Edinburgh', 'Glasgow', 'Dundee'])\n",
    "print(df_new)\n",
    "print('-----')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b70a14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Edinburgh  Glasgow  Dundee\n",
      "a        NaN        1       2\n",
      "b        3.0        4       5\n",
      "c        6.0        7       8\n",
      "-----\n",
      "   Edinburgh  Glasgow  Dundee\n",
      "a       True    False   False\n",
      "b      False    False   False\n",
      "c      False    False   False\n"
     ]
    }
   ],
   "source": [
    "# We will reset one element as NaN (note we have to get that from NumPy)\n",
    "df['Edinburgh']['a']=np.nan\n",
    "print(df)\n",
    "print('-----')\n",
    "\n",
    "# We can then find all the NaN values:\n",
    "print(df.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938190f",
   "metadata": {},
   "source": [
    "Here are some other methods for working with `NaN` which you can find useful:\n",
    "    \n",
    "| Method | Description |\n",
    "| -- | -- |\n",
    "| dropna | Filter axis labels based on whether the values of each label have missing data|\n",
    "| fillna | Fill in missing data with some value |\n",
    "| isnull | Return boolean values indicating which values are missing |\n",
    "| notnull | Negation of isnull |\n",
    "\n",
    "Just like `.drop()`, these methods all return a new object, leaving the original unchanged (this behaviour can be overridden using the argument `inplace=True`).\n",
    "\n",
    "`dropna()` by default removes any row that has any missing value. Here's the example with this applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a52f0b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Edinburgh  Glasgow  Dundee\n",
      "b        3.0        4       5\n",
      "c        6.0        7       8\n"
     ]
    }
   ],
   "source": [
    "print(df.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e613464",
   "metadata": {},
   "source": [
    "We can also drop a row only if all of the entries are `NaN` - here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aca8d37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Edinburgh  Glasgow  Dundee\n",
      "a        NaN      NaN     NaN\n",
      "b        NaN      4.0     5.0\n",
      "c        NaN      7.0     8.0\n",
      "-----\n",
      "Empty DataFrame\n",
      "Columns: [Edinburgh, Glasgow, Dundee]\n",
      "Index: []\n",
      "-----\n",
      "   Edinburgh  Glasgow  Dundee\n",
      "b        NaN      4.0     5.0\n",
      "c        NaN      7.0     8.0\n"
     ]
    }
   ],
   "source": [
    "df['Edinburgh'] = np.nan\n",
    "df.iloc[0] = np.nan\n",
    "\n",
    "print(df)\n",
    "print('-----')\n",
    "\n",
    "print(df.dropna()) # Or write as print(df.dropna(how='any'))\n",
    "print('-----')\n",
    "\n",
    "# Using with additional 'how' argument below: \n",
    "# 'any' : If any NA values are present, drop that row or column.\n",
    "# 'all' : If all values are NA, drop that row or column.\n",
    "print(df.dropna(how='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9e4d9",
   "metadata": {},
   "source": [
    "We could also choose to fill in the `NaN` values with a 0 say using `fillna()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6b83614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Edinburgh  Glasgow  Dundee\n",
      "a        NaN      NaN     NaN\n",
      "b        NaN      4.0     5.0\n",
      "c        NaN      7.0     8.0\n",
      "-----\n",
      "   Edinburgh  Glasgow  Dundee\n",
      "a        0.0      0.0     0.0\n",
      "b        0.0      4.0     5.0\n",
      "c        0.0      7.0     8.0\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "print('-----')\n",
    "\n",
    "print(df.fillna(0))\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb1e88",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "\n",
    "### 1. Removing duplicates\n",
    "Duplicate data can be a serious issue, luckily pandas offers a simple way to remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06a50b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\n",
      "0  a\n",
      "1  b\n",
      "2  c\n",
      "3  d\n",
      "4  c\n",
      "5  b\n",
      "6  a\n",
      "-----\n",
      "   0\n",
      "0  a\n",
      "1  b\n",
      "2  c\n",
      "3  d\n"
     ]
    }
   ],
   "source": [
    "repeat_df = pd.DataFrame(['a','b','c','d','c','b','a'])\n",
    "print(repeat_df)\n",
    "print('-----')\n",
    "\n",
    "print(repeat_df.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a3bbd5",
   "metadata": {},
   "source": [
    "You can also select which rows to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74ce73db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\n",
      "3  d\n",
      "4  c\n",
      "5  b\n",
      "6  a\n"
     ]
    }
   ],
   "source": [
    "print(repeat_df.drop_duplicates(keep='last'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4107d6",
   "metadata": {},
   "source": [
    "### 2. Replacing data\n",
    "We've seen how you can fill in missing non numerical data with `fillna()`. That is actually a special case of more general value replacement. That is done via the `replace()` method.\n",
    "\n",
    "Let's consider an example where the dataset given to us had `-999` as sentinel values for missing data instead of `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20ba201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0\n",
      "0    1.0\n",
      "1 -999.0\n",
      "2    2.0\n",
      "3 -999.0\n",
      "4    3.0\n",
      "5    4.0\n",
      "6 -999.0\n",
      "7 -999.0\n",
      "8    7.0\n",
      "-----\n",
      "     0\n",
      "0  1.0\n",
      "1  NaN\n",
      "2  2.0\n",
      "3  NaN\n",
      "4  3.0\n",
      "5  4.0\n",
      "6  NaN\n",
      "7  NaN\n",
      "8  7.0\n"
     ]
    }
   ],
   "source": [
    "sentinel_df = pd.DataFrame([1., -999., 2., -999., 3., 4., -999, -999, 7.])\n",
    "print(sentinel_df)\n",
    "print('-----')\n",
    "\n",
    "print(sentinel_df.replace(-999, np.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c890c2a8",
   "metadata": {},
   "source": [
    "We could likewise have replaced with another number, or replaced text instead of numbers, and of course we can always make these changes with `inplace=True` as an option to modify our original dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1437aa",
   "metadata": {},
   "source": [
    "### 3. Detecting and Filtering Outliers\n",
    "Filtering or transforming outliers is largely a matter of applying array operations. Consider a dataframe with some normally distributed data (don't worry about how we are generating this - but it is done with NumPy again!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecb64a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0            1\n",
      "count  1000.000000  1000.000000\n",
      "mean     -0.024234    -0.016052\n",
      "std       0.999057     1.010843\n",
      "min      -3.130510    -3.375023\n",
      "25%      -0.654695    -0.715931\n",
      "50%      -0.033217    -0.029855\n",
      "75%       0.671349     0.625514\n",
      "max       3.561907     2.949677\n"
     ]
    }
   ],
   "source": [
    "# Generate two columns of normal (Gaussian, bell-shaped) data with the same parameters) \n",
    "# each with 1000 data points\n",
    "norm_data = pd.DataFrame(np.random.randn(1000, 2))\n",
    "print(norm_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f90b8a",
   "metadata": {},
   "source": [
    "Suppose we now want to find all the values exceeding 1.5 from the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791022c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_data[norm_data > 1.5])\n",
    "# to see it better remove NaN:\n",
    "print(norm_data[norm_data > 1.5].dropna(how='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b51a2",
   "metadata": {},
   "source": [
    "We could set a ceiling for these values at 1.5, by simply replacing them if they exceed that value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f914f695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.049741</td>\n",
       "      <td>-0.051353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.946525</td>\n",
       "      <td>0.941381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.130510</td>\n",
       "      <td>-3.375023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.654695</td>\n",
       "      <td>-0.715931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.033217</td>\n",
       "      <td>-0.029855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.671349</td>\n",
       "      <td>0.625514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1\n",
       "count  1000.000000  1000.000000\n",
       "mean     -0.049741    -0.051353\n",
       "std       0.946525     0.941381\n",
       "min      -3.130510    -3.375023\n",
       "25%      -0.654695    -0.715931\n",
       "50%      -0.033217    -0.029855\n",
       "75%       0.671349     0.625514\n",
       "max       1.500000     1.500000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_data[norm_data > 1.5] = 1.5\n",
    "norm_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82885cb9",
   "metadata": {},
   "source": [
    "- Note the max of each column is now pinned at 1.5 exactly.\n",
    "\n",
    "- What we just did is called **boolean indexing**. Where the condition we specify is `True` we ask pandas to do something, where it is `False` we leave the data alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba85866",
   "metadata": {},
   "source": [
    "### Mini Exercises \n",
    "\n",
    "- Consider the `wood_imports` data set above and explore that is there any missing data or not ? \n",
    "- If there are some missing values, drop those values from the corresponding rows\n",
    "- Investigate and mitigate any duplicated rows for the `wood_imports` data set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b176c",
   "metadata": {},
   "source": [
    "## WHAT is NEXT ?\n",
    "\n",
    "You will see now; \n",
    "\n",
    "- introduction to visualization in python\n",
    "\n",
    "REMINDER\n",
    "\n",
    "- **If you have experienced the Insights Through Data (ITD) module in SEM1, you can recall examples of certain libraries including pandas easily we hope. After checking the examples feel free to skip `Introduction2Pandas.ipynb` file, and focus more on the visualization related files under teh folder, `Set 1 - Introduction2DataViz`.**\n",
    "\n",
    "- **If you are a novice learner and wanted to make further exercises on numpy, pandas, please let us know! Remember these are some basics to warm-up to be familiar with the python syntax, it is still useful for data viz related code writing in python.**\n",
    "\n",
    "- **If you find any error/typo or misleading information on the code scripts, please share your questions/notes via `Coding Support` channel in Teams.** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
